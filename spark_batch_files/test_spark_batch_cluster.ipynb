{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad76e2f5",
   "metadata": {},
   "source": [
    "# Testing the Spark Batch File\n",
    "\n",
    "This spark batch file was tested locally using a smaller version of the crash data stored in the s3 bucket along with the larger file. This jupyter notebook was run in the public docker container image called: `jupyter/pyspark-notebook`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce98a031-2b3f-4708-8199-c466613d7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_ACCESS_KEY_ID: AKIAQ3EGPHEFWG5P2WSX\n",
      "+--------+---+----------+-----------+------------+---+---+---+---+---+---+---+--------------------+\n",
      "|       0|  1|         2|          3|           4|  5|  6|  7|  8|  9| 10| 11|            features|\n",
      "+--------+---+----------+-----------+------------+---+---+---+---+---+---+---+--------------------+\n",
      "|15657177|  N|06/02/2019|32.28023063|-97.74661628|  0|  0|  0|  0|  1|  0|  0|[32.28023063,-97....|\n",
      "|16406486|  N|05/09/2019|33.46381977|-94.41472753|  0|  0|  0|  1|  1|  0|  0|[33.46381977,-94....|\n",
      "|16473665|  N|06/15/2019|30.66068326|-93.89387604|  0|  1|  0|  1|  0|  1|  0|[30.66068326,-93....|\n",
      "|16871051|  N|06/12/2019|33.20390036|-96.59665891|  0|  0|  0|  2|  0|  0|  0|[33.20390036,-96....|\n",
      "|16995273|  N|05/01/2019|29.78455155| -95.5627439|  0|  0|  1|  6|  0|  1|  0|[29.78455155,-95....|\n",
      "|17028441|  N|04/24/2019|32.78551402|-96.91866541|  0|  0|  0|  3|  0|  0|  0|[32.78551402,-96....|\n",
      "|17028486|  N|04/24/2019| 32.9076668|-96.57670095|  0|  0|  0|  0|  2|  0|  0|[32.9076668,-96.5...|\n",
      "|17028493|  N|04/24/2019|29.40451806|-98.61733358|  0|  1|  0|  0|  0|  1|  0|[29.40451806,-98....|\n",
      "|17028502|  N|04/24/2019|32.93153293|-97.23833245|  0|  0|  0|  1|  0|  0|  0|[32.93153293,-97....|\n",
      "|17028509|  N|04/24/2019|33.12445036|-96.65369339|  0|  0|  0|  2|  0|  0|  0|[33.12445036,-96....|\n",
      "|17028575|  N|04/24/2019|32.64436006|-96.84860716|  0|  0|  1|  0|  0|  1|  0|[32.64436006,-96....|\n",
      "|17028622|  N|04/24/2019|32.92493092|-96.78800436|  0|  0|  0|  2|  0|  0|  0|[32.92493092,-96....|\n",
      "|17028626|  N|04/24/2019| 29.2910415|-94.82637608|  0|  0|  0|  3|  0|  0|  0|[29.2910415,-94.8...|\n",
      "|17028636|  N|04/24/2019|31.52705511|-97.13285708|  0|  0|  0|  1|  0|  0|  0|[31.52705511,-97....|\n",
      "|17028669|  N|04/24/2019|32.85687128|-96.83487779|  0|  0|  0|  0|  1|  0|  0|[32.85687128,-96....|\n",
      "|17028674|  N|04/24/2019|30.28181928| -97.5395629|  0|  0|  0|  1|  0|  0|  0|[30.28181928,-97....|\n",
      "|17028676|  N|04/24/2019|27.85738359|-97.62980901|  0|  0|  1|  0|  0|  1|  0|[27.85738359,-97....|\n",
      "|17028679|  N|04/24/2019|32.86390621|-96.83648097|  0|  0|  0|  1|  1|  0|  0|[32.86390621,-96....|\n",
      "|17028689|  N|04/24/2019|32.89951312|-96.88136254|  0|  0|  0|  2|  0|  0|  0|[32.89951312,-96....|\n",
      "|17028722|  N|04/24/2019|32.99873758|-96.99696986|  0|  0|  0|  2|  0|  0|  0|[32.99873758,-96....|\n",
      "+--------+---+----------+-----------+------------+---+---+---+---+---+---+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- 0: string (nullable = true)\n",
      " |-- 1: string (nullable = true)\n",
      " |-- 2: string (nullable = true)\n",
      " |-- 3: double (nullable = true)\n",
      " |-- 4: double (nullable = true)\n",
      " |-- 5: string (nullable = true)\n",
      " |-- 6: string (nullable = true)\n",
      " |-- 7: string (nullable = true)\n",
      " |-- 8: string (nullable = true)\n",
      " |-- 9: string (nullable = true)\n",
      " |-- 10: string (nullable = true)\n",
      " |-- 11: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Append the current working directory to the system path to import the updated script\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import the function from the updated script\n",
    "from src.spark_clustering import load_and_process_crash_data\n",
    "import config\n",
    "\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# Initialize Spark session with AWS credentials\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"CrashDataProcessor\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set logging level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "################################## run batch file on data ##################################\n",
    "\n",
    "def run():\n",
    "    #'s3a://public-crash-data/clean-data/'\n",
    "    S3_OUTPUT_PATH = config.S3_OUTPUT_DIR \n",
    "    S3_OUTPUT_PATH_EDIT = S3_OUTPUT_PATH[:2] + 'a' + S3_OUTPUT_PATH[2:] # add an 'a' to the s3 url\n",
    "    #'s3a://public-crash-data/raw-data/combined_cleaned_group_crash.csv'\n",
    "    S3_INPUT_URL = config.S3_RAW_DATA_URL \n",
    "    S3_INPUT_URL_EDIT = S3_INPUT_URL[:2] + 'a' + S3_INPUT_URL[2:] # add an 'a' to the s3 url\n",
    "    \n",
    "    # load and process kmeans model\n",
    "    crash_data_object = load_and_process_crash_data(spark, S3_INPUT_URL_EDIT)\n",
    "    crash_data_object.assemble_features()\n",
    "    # Run KMeans clustering\n",
    "    crash_data_clustered = crash_data_object.KMeans_model()\n",
    "    \n",
    "    # Compute fatality rate and save results to S3\n",
    "    crash_data_clustered.compute_fatality_rate(\n",
    "        cluster_col='kmeans_cluster',\n",
    "        save_to_s3=True,\n",
    "        s3_path=S3_OUTPUT_PATH_EDIT\n",
    "    )\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
